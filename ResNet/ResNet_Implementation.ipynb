{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3N43-kWKy_f",
        "outputId": "b268e243-14df-406d-a585-09b0914e7497"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8aP_RdSo7qiu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "from collections import OrderedDict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, datasets\n",
        "from torchsummary import summary\n",
        "from torch.utils.data import Dataset, DataLoader, random_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#### Lambda Layer\n",
        "- **Purpose:** This layer allows you to define custom operations or transformations within the network. This is useful for operations that aren't standard layers, like identity mapping with zero-padding.\n"
      ],
      "metadata": {
        "id": "G7CVrFJu-UQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class LambdaLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    This class defines a Lambda Layer, which applies a custom function to the input tensor.\n",
        "\n",
        "    Attributes:\n",
        "        lambd (function): A function that defines the operation to be applied to the input tensor.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, lambd):\n",
        "        \"\"\"\n",
        "        Initialize the Lambda Layer.\n",
        "\n",
        "        Args:\n",
        "            lambd (function): The function that defines the operation to be applied to the input tensor.\n",
        "        \"\"\"\n",
        "        super(LambdaLayer, self).__init__()\n",
        "        self.lambd = lambd\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass for the Lambda Layer.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): The input tensor.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The output tensor after applying the custom function.\n",
        "        \"\"\"\n",
        "        return self.lambd(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "T8Svb9fj74Me"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  \n",
        "#### Basic Convolution Block\n",
        "- **Components:**\n",
        "  1. **Convolutional Layers:** Two convolutional layers that reduce and then restore the spatial dimensions of the input tensor.\n",
        "  2. **Batch Normalization:** Applied after each convolutional layer to stabilize the learning process.\n",
        "  3. **ReLU Activation:** Non-linear activation function that introduces non-linearity into the model.\n",
        "  4. **Shortcut Connection:** Ensures that the input and output dimensions match. It adds the input directly to the output of the block, which helps in training deeper networks by mitigating the vanishing gradient problem.\n",
        "  \n",
        "- **Shortcut Connection Options:**\n",
        "  - **Option A:** Uses an identity shortcut with zero-padding to increase the channel dimensions when required.\n",
        "  - **Option B:** Uses a 1x1 convolution to adjust the dimensions directly (projection shortcut). This is more computationally expensive but often more effective in deeper networks.\n",
        "\n",
        "This implementation ensures that the model is flexible and can handle different cases of input-output dimension mismatches while maintaining computational efficiency."
      ],
      "metadata": {
        "id": "r35SNceZ_QBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicConvBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    This class defines a Basic Convolution Block for ResNet.\n",
        "\n",
        "    The block consists of two convolutional layers followed by Batch Normalization and ReLU activation.\n",
        "    Additionally, a shortcut connection is added to ensure the input and output dimensions match.\n",
        "\n",
        "    Attributes:\n",
        "        features (nn.Sequential): The sequence of convolutional layers, Batch Normalization, and ReLU activations.\n",
        "        shortcut (nn.Sequential or LambdaLayer): The shortcut connection to match the dimensions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1, option='A'):\n",
        "        \"\"\"\n",
        "        Initialize the Basic Convolution Block.\n",
        "\n",
        "        Args:\n",
        "            in_channels (int): Number of input channels.\n",
        "            out_channels (int): Number of output channels.\n",
        "            stride (int, optional): Stride for the convolution operation. Default is 1.\n",
        "            option (str, optional): Option for the shortcut connection to match dimensions. 'A' or 'B'. Default is 'A'.\n",
        "        \"\"\"\n",
        "        super(BasicConvBlock, self).__init__()\n",
        "\n",
        "        # Define the sequence of convolutional layers, Batch Normalization, and ReLU activation.\n",
        "        self.features = nn.Sequential(OrderedDict([\n",
        "            ('conv1', nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)),\n",
        "            ('bn1', nn.BatchNorm2d(out_channels)),\n",
        "            ('relu1', nn.ReLU(inplace=True)),\n",
        "            ('conv2', nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)),\n",
        "            ('bn2', nn.BatchNorm2d(out_channels))\n",
        "        ]))\n",
        "\n",
        "        # Initialize an empty shortcut connection by default.\n",
        "        self.shortcut = nn.Sequential()\n",
        "\n",
        "        # If the input and output dimensions do not match, apply the appropriate shortcut connection.\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            if option == 'A':\n",
        "                # Option A: Use identity shortcuts with zero padding to match dimensions.\n",
        "                pad_to_add = (out_channels - in_channels) // 2\n",
        "                self.shortcut = LambdaLayer(lambda x: F.pad(\n",
        "                    x[:, :, ::stride, ::stride],\n",
        "                    (0, 0, 0, 0, pad_to_add, pad_to_add, 0, 0)\n",
        "                ))\n",
        "            elif option == 'B':\n",
        "                # Option B: Use 1x1 convolution to match dimensions (projection shortcut).\n",
        "                self.shortcut = nn.Sequential(OrderedDict([\n",
        "                    ('s_conv', nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)),\n",
        "                    ('s_bn', nn.BatchNorm2d(out_channels))\n",
        "                ]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass for the Basic Convolution Block.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): The input tensor.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The output tensor after applying the convolutional layers and the shortcut connection.\n",
        "        \"\"\"\n",
        "        out = self.features(x)\n",
        "        out += self.shortcut(x)  # Add the shortcut connection output to the features output.\n",
        "        out = F.relu(out)  # Apply ReLU activation to the combined output.\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "susmuVPv7u0P"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding `F.pad` and the ResNet Code\n",
        "\n",
        "The `F.pad` function in PyTorch is used to apply padding to a tensor. In the context of a ResNet architecture, padding is often applied to match the dimensions of the input and output tensors when using shortcut connections (also known as skip connections).\n",
        "\n",
        "Let's break down the line of code and understand how `F.pad` works in this context:\n",
        "\n",
        "```python\n",
        "F.pad(x[:, :, ::2, ::2], (0,0, 0,0, pad,pad, 0,0))\n",
        "```\n",
        "\n",
        "### 1. **Input Tensor**\n",
        "The input tensor `x` is assumed to be a 4D tensor with dimensions `[batch_size, channels, height, width]`, typical for image data.\n",
        "\n",
        "- `x[:, :, ::2, ::2]`:\n",
        "  - This slicing operation down-samples the tensor by taking every second element in both the height and width dimensions (`::2`).\n",
        "  - The result is a tensor with reduced spatial dimensions, effectively down-sampling the image by a factor of 2.\n",
        "\n",
        "### 2. **Understanding the Padding in `F.pad`**\n",
        "`F.pad` requires the padding amounts for each dimension of the tensor. The padding values are specified in reverse order, with two values for each dimension (one for padding at the beginning and one for padding at the end).\n",
        "\n",
        "- **Padding Values**: `(0, 0, 0, 0, pad, pad, 0, 0)`\n",
        "  - The padding is applied as follows:\n",
        "    - **(0, 0)**: No padding is applied to the last dimension (batch dimension).\n",
        "    - **(pad, pad)**: Padding is applied to the channel dimension. `pad` is added at both the beginning and end of the channels dimension.\n",
        "    - **(0, 0)**: No padding is applied to the spatial dimensions (height and width).\n",
        "\n",
        "### 3. **Padding Explained**\n",
        "The specific padding order corresponds to the dimensions in the tensor:\n",
        "\n",
        "- **Width (Spatial dimension)**: `(0, 0)` means no padding is applied to the width.\n",
        "- **Height (Spatial dimension)**: `(0, 0)` means no padding is applied to the height.\n",
        "- **Channels**: `(pad, pad)` adds `pad` number of channels at both the beginning and end of the channels dimension.\n",
        "- **Batch**: `(0, 0)` means no padding is applied to the batch dimension.\n",
        "\n",
        "### 4. **Example of `F.pad` in Context**\n",
        "Assume we have an input tensor `x` with shape `[batch_size, 64, 16, 16]` (i.e., batch size of `batch_size`, 64 channels, and spatial dimensions 16x16).\n",
        "\n",
        "- After applying `x[:, :, ::2, ::2]`, the tensor would have dimensions `[batch_size, 64, 8, 8]`, reducing the height and width by half.\n",
        "- Now, if `pad = 32`, the `F.pad` function would add 32 channels on both sides of the channel dimension.\n",
        "- The resulting tensor would have dimensions `[batch_size, 128, 8, 8]`, where 128 is the result of 64 original channels plus 32 padded channels at the beginning and 32 padded channels at the end.\n",
        "\n",
        "### 5. **Use Case in ResNet**\n",
        "In the context of a ResNet, if the input and output channels differ, padding is necessary to match the dimensions so that the input can be added directly to the output of the residual block.\n",
        "\n",
        "- **Option A (Identity Shortcut with Zero Padding)**: The channel dimension is increased by adding zero-padding, ensuring that the input and output tensors have the same number of channels.\n",
        "- **Option B (1x1 Convolution Shortcut)**: Instead of padding, a 1x1 convolution is used to project the input tensor to the desired number of channels.\n",
        "\n",
        "This padding ensures that when the input tensor is added to the output of the residual block, the dimensions align correctly, allowing for the addition operation to take place without errors.\n",
        "\n",
        "\n",
        "- **`F.pad`**: Pads the tensor to ensure the input and output dimensions match in the ResNet architecture.\n",
        "- **Zero Padding**: Adds zeros to increase the number of channels.\n",
        "- **Slicing (`::2`)**: Down-samples the spatial dimensions by a factor of 2.\n",
        "- **Use in ResNet**: Facilitates the shortcut connection when the number of channels differs between the input and output.\n",
        "\n"
      ],
      "metadata": {
        "id": "KF_8-rD2DL45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from collections import OrderedDict\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    \"\"\"\n",
        "    ResNet-56 architecture for CIFAR-10 Dataset of shape 32x32x3.\n",
        "\n",
        "    Args:\n",
        "        block_type (nn.Module): The type of residual block to use.\n",
        "        num_blocks (list): List containing the number of blocks for each layer.\n",
        "        num_classes (int, optional): Number of output classes. Default is 10.\n",
        "\n",
        "    Attributes:\n",
        "        in_channels (int): Number of input channels.\n",
        "        conv0 (nn.Conv2d): Initial convolutional layer.\n",
        "        bn0 (nn.BatchNorm2d): Batch normalization layer.\n",
        "        block1 (nn.Sequential): First block layer.\n",
        "        block2 (nn.Sequential): Second block layer.\n",
        "        block3 (nn.Sequential): Third block layer.\n",
        "        avgpool (nn.AdaptiveAvgPool2d): Adaptive average pooling layer.\n",
        "        linear (nn.Linear): Linear layer for classification.\n",
        "    \"\"\"\n",
        "    def __init__(self, block_type, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "\n",
        "        self.in_channels = 16  # Initial number of channels\n",
        "\n",
        "        # Initial convolutional and batch normalization layers\n",
        "        self.conv0 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn0 = nn.BatchNorm2d(16)\n",
        "\n",
        "        # Residual blocks for each layer\n",
        "        self.block1 = self.__build_layer(block_type, 16, num_blocks[0], starting_stride=1)\n",
        "        self.block2 = self.__build_layer(block_type, 32, num_blocks[1], starting_stride=2)\n",
        "        self.block3 = self.__build_layer(block_type, 64, num_blocks[2], starting_stride=2)\n",
        "\n",
        "        # Adaptive average pooling and linear layer for classification\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.linear = nn.Linear(64, num_classes)\n",
        "\n",
        "        # Initialize weights\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def __build_layer(self, block_type, out_channels, num_blocks, starting_stride):\n",
        "        \"\"\"\n",
        "        Build a layer consisting of multiple residual blocks.\n",
        "\n",
        "        Args:\n",
        "            block_type (nn.Module): The type of residual block to use.\n",
        "            out_channels (int): Number of output channels.\n",
        "            num_blocks (int): Number of blocks in the layer.\n",
        "            starting_stride (int): Stride value for the first block.\n",
        "\n",
        "        Returns:\n",
        "            nn.Sequential: Sequential container of the residual blocks.\n",
        "        \"\"\"\n",
        "        strides_list = [starting_stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "\n",
        "        for stride in strides_list:\n",
        "            layers.append(block_type(self.in_channels, out_channels, stride))\n",
        "            self.in_channels = out_channels  # Update in_channels for the next block\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the ResNet model.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor.\n",
        "        \"\"\"\n",
        "        # Initial convolution and normalization\n",
        "        out = F.relu(self.bn0(self.conv0(x)))\n",
        "\n",
        "        # Pass through residual blocks\n",
        "        out = self.block1(out)\n",
        "        out = self.block2(out)\n",
        "        out = self.block3(out)\n",
        "\n",
        "        # Global average pooling and final classification layer\n",
        "        out = self.avgpool(out)\n",
        "        out = torch.flatten(out, 1)\n",
        "        out = self.linear(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        \"\"\"\n",
        "        Initialize the weights of the model using He initialization.\n",
        "        \"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n"
      ],
      "metadata": {
        "id": "ktloi4rr-St-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ResNet56():\n",
        "    return ResNet(block_type=BasicConvBlock, num_blocks=[9,9,9])\n",
        "\n",
        "model = ResNet56()\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "summary(model, (3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KgfZ9wVKOF8",
        "outputId": "84a0c4f8-608f-44e7-a90c-68ca6a939ad5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 32, 32]             432\n",
            "       BatchNorm2d-2           [-1, 16, 32, 32]              32\n",
            "            Conv2d-3           [-1, 16, 32, 32]           2,304\n",
            "       BatchNorm2d-4           [-1, 16, 32, 32]              32\n",
            "              ReLU-5           [-1, 16, 32, 32]               0\n",
            "            Conv2d-6           [-1, 16, 32, 32]           2,304\n",
            "       BatchNorm2d-7           [-1, 16, 32, 32]              32\n",
            "    BasicConvBlock-8           [-1, 16, 32, 32]               0\n",
            "            Conv2d-9           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-10           [-1, 16, 32, 32]              32\n",
            "             ReLU-11           [-1, 16, 32, 32]               0\n",
            "           Conv2d-12           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-13           [-1, 16, 32, 32]              32\n",
            "   BasicConvBlock-14           [-1, 16, 32, 32]               0\n",
            "           Conv2d-15           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-16           [-1, 16, 32, 32]              32\n",
            "             ReLU-17           [-1, 16, 32, 32]               0\n",
            "           Conv2d-18           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-19           [-1, 16, 32, 32]              32\n",
            "   BasicConvBlock-20           [-1, 16, 32, 32]               0\n",
            "           Conv2d-21           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-22           [-1, 16, 32, 32]              32\n",
            "             ReLU-23           [-1, 16, 32, 32]               0\n",
            "           Conv2d-24           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-25           [-1, 16, 32, 32]              32\n",
            "   BasicConvBlock-26           [-1, 16, 32, 32]               0\n",
            "           Conv2d-27           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-28           [-1, 16, 32, 32]              32\n",
            "             ReLU-29           [-1, 16, 32, 32]               0\n",
            "           Conv2d-30           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-31           [-1, 16, 32, 32]              32\n",
            "   BasicConvBlock-32           [-1, 16, 32, 32]               0\n",
            "           Conv2d-33           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-34           [-1, 16, 32, 32]              32\n",
            "             ReLU-35           [-1, 16, 32, 32]               0\n",
            "           Conv2d-36           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-37           [-1, 16, 32, 32]              32\n",
            "   BasicConvBlock-38           [-1, 16, 32, 32]               0\n",
            "           Conv2d-39           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-40           [-1, 16, 32, 32]              32\n",
            "             ReLU-41           [-1, 16, 32, 32]               0\n",
            "           Conv2d-42           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-43           [-1, 16, 32, 32]              32\n",
            "   BasicConvBlock-44           [-1, 16, 32, 32]               0\n",
            "           Conv2d-45           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-46           [-1, 16, 32, 32]              32\n",
            "             ReLU-47           [-1, 16, 32, 32]               0\n",
            "           Conv2d-48           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-49           [-1, 16, 32, 32]              32\n",
            "   BasicConvBlock-50           [-1, 16, 32, 32]               0\n",
            "           Conv2d-51           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-52           [-1, 16, 32, 32]              32\n",
            "             ReLU-53           [-1, 16, 32, 32]               0\n",
            "           Conv2d-54           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-55           [-1, 16, 32, 32]              32\n",
            "   BasicConvBlock-56           [-1, 16, 32, 32]               0\n",
            "           Conv2d-57           [-1, 32, 16, 16]           4,608\n",
            "      BatchNorm2d-58           [-1, 32, 16, 16]              64\n",
            "             ReLU-59           [-1, 32, 16, 16]               0\n",
            "           Conv2d-60           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-61           [-1, 32, 16, 16]              64\n",
            "      LambdaLayer-62           [-1, 32, 16, 16]               0\n",
            "   BasicConvBlock-63           [-1, 32, 16, 16]               0\n",
            "           Conv2d-64           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-65           [-1, 32, 16, 16]              64\n",
            "             ReLU-66           [-1, 32, 16, 16]               0\n",
            "           Conv2d-67           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-68           [-1, 32, 16, 16]              64\n",
            "   BasicConvBlock-69           [-1, 32, 16, 16]               0\n",
            "           Conv2d-70           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-71           [-1, 32, 16, 16]              64\n",
            "             ReLU-72           [-1, 32, 16, 16]               0\n",
            "           Conv2d-73           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-74           [-1, 32, 16, 16]              64\n",
            "   BasicConvBlock-75           [-1, 32, 16, 16]               0\n",
            "           Conv2d-76           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-77           [-1, 32, 16, 16]              64\n",
            "             ReLU-78           [-1, 32, 16, 16]               0\n",
            "           Conv2d-79           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-80           [-1, 32, 16, 16]              64\n",
            "   BasicConvBlock-81           [-1, 32, 16, 16]               0\n",
            "           Conv2d-82           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-83           [-1, 32, 16, 16]              64\n",
            "             ReLU-84           [-1, 32, 16, 16]               0\n",
            "           Conv2d-85           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-86           [-1, 32, 16, 16]              64\n",
            "   BasicConvBlock-87           [-1, 32, 16, 16]               0\n",
            "           Conv2d-88           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-89           [-1, 32, 16, 16]              64\n",
            "             ReLU-90           [-1, 32, 16, 16]               0\n",
            "           Conv2d-91           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-92           [-1, 32, 16, 16]              64\n",
            "   BasicConvBlock-93           [-1, 32, 16, 16]               0\n",
            "           Conv2d-94           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-95           [-1, 32, 16, 16]              64\n",
            "             ReLU-96           [-1, 32, 16, 16]               0\n",
            "           Conv2d-97           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-98           [-1, 32, 16, 16]              64\n",
            "   BasicConvBlock-99           [-1, 32, 16, 16]               0\n",
            "          Conv2d-100           [-1, 32, 16, 16]           9,216\n",
            "     BatchNorm2d-101           [-1, 32, 16, 16]              64\n",
            "            ReLU-102           [-1, 32, 16, 16]               0\n",
            "          Conv2d-103           [-1, 32, 16, 16]           9,216\n",
            "     BatchNorm2d-104           [-1, 32, 16, 16]              64\n",
            "  BasicConvBlock-105           [-1, 32, 16, 16]               0\n",
            "          Conv2d-106           [-1, 32, 16, 16]           9,216\n",
            "     BatchNorm2d-107           [-1, 32, 16, 16]              64\n",
            "            ReLU-108           [-1, 32, 16, 16]               0\n",
            "          Conv2d-109           [-1, 32, 16, 16]           9,216\n",
            "     BatchNorm2d-110           [-1, 32, 16, 16]              64\n",
            "  BasicConvBlock-111           [-1, 32, 16, 16]               0\n",
            "          Conv2d-112             [-1, 64, 8, 8]          18,432\n",
            "     BatchNorm2d-113             [-1, 64, 8, 8]             128\n",
            "            ReLU-114             [-1, 64, 8, 8]               0\n",
            "          Conv2d-115             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-116             [-1, 64, 8, 8]             128\n",
            "     LambdaLayer-117             [-1, 64, 8, 8]               0\n",
            "  BasicConvBlock-118             [-1, 64, 8, 8]               0\n",
            "          Conv2d-119             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-120             [-1, 64, 8, 8]             128\n",
            "            ReLU-121             [-1, 64, 8, 8]               0\n",
            "          Conv2d-122             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-123             [-1, 64, 8, 8]             128\n",
            "  BasicConvBlock-124             [-1, 64, 8, 8]               0\n",
            "          Conv2d-125             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-126             [-1, 64, 8, 8]             128\n",
            "            ReLU-127             [-1, 64, 8, 8]               0\n",
            "          Conv2d-128             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-129             [-1, 64, 8, 8]             128\n",
            "  BasicConvBlock-130             [-1, 64, 8, 8]               0\n",
            "          Conv2d-131             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-132             [-1, 64, 8, 8]             128\n",
            "            ReLU-133             [-1, 64, 8, 8]               0\n",
            "          Conv2d-134             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-135             [-1, 64, 8, 8]             128\n",
            "  BasicConvBlock-136             [-1, 64, 8, 8]               0\n",
            "          Conv2d-137             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-138             [-1, 64, 8, 8]             128\n",
            "            ReLU-139             [-1, 64, 8, 8]               0\n",
            "          Conv2d-140             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-141             [-1, 64, 8, 8]             128\n",
            "  BasicConvBlock-142             [-1, 64, 8, 8]               0\n",
            "          Conv2d-143             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-144             [-1, 64, 8, 8]             128\n",
            "            ReLU-145             [-1, 64, 8, 8]               0\n",
            "          Conv2d-146             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-147             [-1, 64, 8, 8]             128\n",
            "  BasicConvBlock-148             [-1, 64, 8, 8]               0\n",
            "          Conv2d-149             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-150             [-1, 64, 8, 8]             128\n",
            "            ReLU-151             [-1, 64, 8, 8]               0\n",
            "          Conv2d-152             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-153             [-1, 64, 8, 8]             128\n",
            "  BasicConvBlock-154             [-1, 64, 8, 8]               0\n",
            "          Conv2d-155             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-156             [-1, 64, 8, 8]             128\n",
            "            ReLU-157             [-1, 64, 8, 8]               0\n",
            "          Conv2d-158             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-159             [-1, 64, 8, 8]             128\n",
            "  BasicConvBlock-160             [-1, 64, 8, 8]               0\n",
            "          Conv2d-161             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-162             [-1, 64, 8, 8]             128\n",
            "            ReLU-163             [-1, 64, 8, 8]               0\n",
            "          Conv2d-164             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-165             [-1, 64, 8, 8]             128\n",
            "  BasicConvBlock-166             [-1, 64, 8, 8]               0\n",
            "AdaptiveAvgPool2d-167             [-1, 64, 1, 1]               0\n",
            "          Linear-168                   [-1, 10]             650\n",
            "================================================================\n",
            "Total params: 853,018\n",
            "Trainable params: 853,018\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 12.16\n",
            "Params size (MB): 3.25\n",
            "Estimated Total Size (MB): 15.42\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the CIFAR 10 Dataset"
      ],
      "metadata": {
        "id": "CYjJZyf8LfDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "def dataloader_cifar():\n",
        "    \"\"\"\n",
        "    Create dataloaders for the CIFAR-10 dataset.\n",
        "\n",
        "    Returns:\n",
        "        train_loader (torch.utils.data.DataLoader): Dataloader for the training set.\n",
        "        val_loader (torch.utils.data.DataLoader): Dataloader for the validation set.\n",
        "        test_loader (torch.utils.data.DataLoader): Dataloader for the test set.\n",
        "    \"\"\"\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    ])\n",
        "\n",
        "    # Input Data in Google Drive (Update path if needed)\n",
        "    train_dataset = datasets.CIFAR10('/content/drive/MyDrive/All_Datasets/CIFAR10', train=True, download=True, transform=transform)\n",
        "    test_dataset = datasets.CIFAR10('/content/drive/MyDrive/All_Datasets/CIFAR10', train=False, download=True, transform=transform)\n",
        "\n",
        "    # Split dataset into training set and validation set\n",
        "    train_dataset, val_dataset = random_split(train_dataset, [45000, 5000])\n",
        "\n",
        "    print(\"Image shape of a random sample image : {}\".format(train_dataset[0][0].numpy().shape), end='\\n\\n')\n",
        "    print(\"Training Set:   {} images\".format(len(train_dataset)))\n",
        "    print(\"Validation Set:   {} images\".format(len(val_dataset)))\n",
        "    print(\"Test Set:       {} images\".format(len(test_dataset)))\n",
        "\n",
        "    BATCH_SIZE = 32\n",
        "\n",
        "    # Generate dataloaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=10000, shuffle=False)\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "train_loader, val_loader, test_loader = dataloader_cifar()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZXTQsyNKQEm",
        "outputId": "aa5e6ad5-09ef-4cc1-fbd8-3c53b4e6ccf7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Image shape of a random sample image : (3, 32, 32)\n",
            "\n",
            "Training Set:   45000 images\n",
            "Validation Set:   5000 images\n",
            "Test Set:       10000 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a detailed breakdown of your training function, formatted in markdown with explanations on what each part does and why it's important:\n",
        "\n",
        "---\n",
        "\n",
        "## Training Function Breakdown\n",
        "\n",
        "### 1. **Imports and Setup**\n",
        "\n",
        "\n",
        "\n",
        "- **`criterion`**: Defines the loss function. `nn.CrossEntropyLoss` is commonly used for classification tasks. It combines `nn.LogSoftmax()` and `nn.NLLLoss()` in one single class.\n",
        "- **`optimizer`**: Sets up the optimizer. `optim.Adam` is a popular optimization algorithm that adjusts the learning rate based on the first and second moments of the gradients.\n"
      ],
      "metadata": {
        "id": "JiQIITpcMTQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n"
      ],
      "metadata": {
        "id": "QAZKR6ADLa8E"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 2. **Training Function Definition**\n",
        "\n",
        "\n",
        "- **`EPOCHS`**: The number of times the entire dataset will pass through the model.\n",
        "- **`train_samples_num` and `val_samples_num`**: Number of samples in the training and validation sets respectively. These values are used to compute the average loss and accuracy.\n",
        "- **`train_costs` and `val_costs`**: Lists to store the training and validation losses for each epoch.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### 3. **Training Phase**\n",
        "\n",
        "- **`model.train().cuda()`**: Sets the model to training mode and moves it to GPU (if available).\n",
        "- **`optimizer.zero_grad()`**: Clears old gradients from the last step.\n",
        "- **`prediction = model(inputs)`**: Performs the forward pass.\n",
        "- **`loss.backward()`**: Computes the gradient of the loss with respect to model parameters.\n",
        "- **`optimizer.step()`**: Updates the model parameters using the computed gradients.\n",
        "- **`_, predicted_outputs = torch.max(prediction.data, 1)`**: Gets the predicted class labels.\n",
        "- **`correct_train += (predicted_outputs == labels).float().sum().item()`**: Counts correct predictions.\n",
        "- **`train_running_loss += (loss.data.item() * inputs.shape[0])`**: Accumulates the loss for averaging later.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Validation Phase**\n",
        "\n",
        "\n",
        "- **`model.eval().cuda()`**: Sets the model to evaluation mode and moves it to GPU.\n",
        "- **`with torch.no_grad()`**: Disables gradient calculation, which saves memory and computations during evaluation.\n",
        "- **`val_running_loss += (loss.data.item() * inputs.shape[0])`**: Accumulates the validation loss.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "h4sBJTRrMdKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model():\n",
        "    EPOCHS = 15\n",
        "    train_samples_num = 45000\n",
        "    val_samples_num = 5000\n",
        "    train_costs, val_costs = [], []\n",
        "\n",
        "    #Training phase.\n",
        "    for epoch in range(EPOCHS):\n",
        "\n",
        "        train_running_loss = 0\n",
        "        correct_train = 0\n",
        "\n",
        "        model.train().cuda()\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            \"\"\" for every mini-batch during the training phase, we typically want to explicitly set the gradients\n",
        "            to zero before starting to do backpropragation \"\"\"\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Start the forward pass\n",
        "            prediction = model(inputs)\n",
        "\n",
        "            loss = criterion(prediction, labels)\n",
        "\n",
        "            # do backpropagation and update weights with step()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print('outputs on which to apply torch.max ', prediction)\n",
        "            # find the maximum along the rows, use dim=1 to torch.max()\n",
        "            _, predicted_outputs = torch.max(prediction.data, 1)\n",
        "\n",
        "            # Update the running corrects\n",
        "            correct_train += (predicted_outputs == labels).float().sum().item()\n",
        "\n",
        "            ''' Compute batch loss\n",
        "            multiply each average batch loss with batch-length.\n",
        "            The batch-length is inputs.size(0) which gives the number total images in each batch.\n",
        "            Essentially I am un-averaging the previously calculated Loss '''\n",
        "            train_running_loss += (loss.data.item() * inputs.shape[0])\n",
        "\n",
        "\n",
        "        train_epoch_loss = train_running_loss / train_samples_num\n",
        "\n",
        "        train_costs.append(train_epoch_loss)\n",
        "\n",
        "        train_acc =  correct_train / train_samples_num\n",
        "\n",
        "        # Now check trained weights on the validation set\n",
        "        val_running_loss = 0\n",
        "        correct_val = 0\n",
        "\n",
        "        model.eval().cuda()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                # Forward pass.\n",
        "                prediction = model(inputs)\n",
        "\n",
        "                # Compute the loss.\n",
        "                loss = criterion(prediction, labels)\n",
        "\n",
        "                # Compute validation accuracy.\n",
        "                _, predicted_outputs = torch.max(prediction.data, 1)\n",
        "                correct_val += (predicted_outputs == labels).float().sum().item()\n",
        "\n",
        "            # Compute batch loss.\n",
        "            val_running_loss += (loss.data.item() * inputs.shape[0])\n",
        "\n",
        "            val_epoch_loss = val_running_loss / val_samples_num\n",
        "            val_costs.append(val_epoch_loss)\n",
        "            val_acc =  correct_val / val_samples_num\n",
        "\n",
        "        info = \"[Epoch {}/{}]: train-loss = {:0.6f} | train-acc = {:0.3f} | val-loss = {:0.6f} | val-acc = {:0.3f}\"\n",
        "\n",
        "        print(info.format(epoch+1, EPOCHS, train_epoch_loss, train_acc, val_epoch_loss, val_acc))\n",
        "\n",
        "        torch.save(model.state_dict(), '/content/checkpoint_gpu_{}'.format(epoch + 1))\n",
        "\n",
        "    torch.save(model.state_dict(), '/content/resnet-56_weights_gpu')\n",
        "\n",
        "    return train_costs, val_costs\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "11g7vleLMrJ-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. **Logging and Saving the Model**\n",
        "\n",
        "\n",
        "- **`Logging`**: Prints the training and validation losses and accuracies for each epoch.\n",
        "- **`torch.save`**: Saves the model weights. Saving the model at each epoch helps to recover from potential interruptions and monitor the model's progress.\n"
      ],
      "metadata": {
        "id": "DLnj16WsM4WR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pwd\n",
        "train_costs, val_costs = train_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vN9n-vTPPGU4",
        "outputId": "f9dd1cb1-6a38-4081-8f1d-0528c1c64005"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1/15]: train-loss = 1.646446 | train-acc = 0.380 | val-loss = 0.001274 | val-acc = 0.512\n",
            "[Epoch 2/15]: train-loss = 1.103993 | train-acc = 0.603 | val-loss = 0.000800 | val-acc = 0.657\n",
            "[Epoch 3/15]: train-loss = 0.827500 | train-acc = 0.709 | val-loss = 0.000243 | val-acc = 0.694\n",
            "[Epoch 4/15]: train-loss = 0.679794 | train-acc = 0.763 | val-loss = 0.000091 | val-acc = 0.770\n",
            "[Epoch 5/15]: train-loss = 0.574650 | train-acc = 0.801 | val-loss = 0.000653 | val-acc = 0.776\n",
            "[Epoch 6/15]: train-loss = 0.489664 | train-acc = 0.831 | val-loss = 0.000556 | val-acc = 0.798\n",
            "[Epoch 7/15]: train-loss = 0.418733 | train-acc = 0.854 | val-loss = 0.000457 | val-acc = 0.809\n",
            "[Epoch 8/15]: train-loss = 0.352605 | train-acc = 0.877 | val-loss = 0.000038 | val-acc = 0.801\n",
            "[Epoch 9/15]: train-loss = 0.301473 | train-acc = 0.894 | val-loss = 0.000097 | val-acc = 0.803\n",
            "[Epoch 10/15]: train-loss = 0.250305 | train-acc = 0.913 | val-loss = 0.000025 | val-acc = 0.810\n",
            "[Epoch 11/15]: train-loss = 0.215240 | train-acc = 0.923 | val-loss = 0.000467 | val-acc = 0.809\n",
            "[Epoch 12/15]: train-loss = 0.184395 | train-acc = 0.934 | val-loss = 0.000055 | val-acc = 0.820\n",
            "[Epoch 13/15]: train-loss = 0.158073 | train-acc = 0.944 | val-loss = 0.000029 | val-acc = 0.810\n",
            "[Epoch 14/15]: train-loss = 0.143677 | train-acc = 0.948 | val-loss = 0.000070 | val-acc = 0.806\n",
            "[Epoch 15/15]: train-loss = 0.128370 | train-acc = 0.955 | val-loss = 0.000009 | val-acc = 0.805\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Restore the model.\n",
        "model = ResNet56()\n",
        "model.load_state_dict(torch.load('/content/resnet-56_weights_gpu'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "072mc3rSPGyb",
        "outputId": "bd91f716-173c-427d-802a-9383fcb6a0cb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "EFGX0nVhTwIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_samples_num = 10000\n",
        "correct = 0\n",
        "\n",
        "model.eval().cuda()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        # Make predictions.\n",
        "        prediction = model(inputs)\n",
        "\n",
        "        # Retrieve predictions indexes.\n",
        "        _, predicted_class = torch.max(prediction.data, 1)\n",
        "\n",
        "        # Compute number of correct predictions.\n",
        "        correct += (predicted_class == labels).float().sum().item()\n",
        "\n",
        "test_accuracy = correct / test_samples_num\n",
        "print('Test accuracy: {}'.format(test_accuracy))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVVpwv_qTYL8",
        "outputId": "2d93d272-4541-4fa2-80e9-95719f35af9c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.8046\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation on a Custom Dataset\n",
        "\n",
        "[Link to Jelly Fish Dataset](https://www.kaggle.com/datasets/anshtanwar/jellyfish-types)"
      ],
      "metadata": {
        "id": "H17Jk68gVEZ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "def dataloader_jellyfish():\n",
        "    \"\"\"\n",
        "    Create dataloaders for the Jelly-Fish dataset.\n",
        "\n",
        "    Returns:\n",
        "        train_loader (torch.utils.data.DataLoader): Dataloader for the training set.\n",
        "        val_loader (torch.utils.data.DataLoader): Dataloader for the validation set.\n",
        "        test_loader (torch.utils.data.DataLoader): Dataloader for the test set.\n",
        "    \"\"\"\n",
        "    # Define transformations for the dataset\n",
        "    transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "\n",
        "    # Load the dataset\n",
        "    dataset = datasets.ImageFolder('/content/drive/MyDrive/All_Datasets/Jelly-Fish', transform=transform)\n",
        "\n",
        "    # Split the dataset into training, validation, and test sets\n",
        "    train_size = int(0.7 * len(dataset))\n",
        "    val_size = int(0.15 * len(dataset))\n",
        "    test_size = len(dataset) - train_size - val_size\n",
        "    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "    print(f\"Training Set: {len(train_dataset)} images\")\n",
        "    print(f\"Validation Set: {len(val_dataset)} images\")\n",
        "    print(f\"Test Set: {len(test_dataset)} images\")\n",
        "\n",
        "    BATCH_SIZE = 32\n",
        "\n",
        "    # Generate dataloaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "train_loader, val_loader, test_loader = dataloader_jellyfish()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2VDR0WVVD5Q",
        "outputId": "3521ded6-c582-49b4-f691-ac446cb2a3c5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Set: 630 images\n",
            "Validation Set: 135 images\n",
            "Test Set: 135 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ResNet56(num_classes=6):\n",
        "    return ResNet(block_type=BasicConvBlock, num_blocks=[9, 9, 9], num_classes=num_classes)\n",
        "\n",
        "\n",
        "model = ResNet56()\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "summary(model, (3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BUcs_TJT1xu",
        "outputId": "dc939062-dff2-409d-e2cf-7d49abad3a1e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 32, 32]             432\n",
            "       BatchNorm2d-2           [-1, 16, 32, 32]              32\n",
            "            Conv2d-3           [-1, 16, 32, 32]           2,304\n",
            "       BatchNorm2d-4           [-1, 16, 32, 32]              32\n",
            "              ReLU-5           [-1, 16, 32, 32]               0\n",
            "            Conv2d-6           [-1, 16, 32, 32]           2,304\n",
            "       BatchNorm2d-7           [-1, 16, 32, 32]              32\n",
            "    BasicConvBlock-8           [-1, 16, 32, 32]               0\n",
            "            Conv2d-9           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-10           [-1, 16, 32, 32]              32\n",
            "             ReLU-11           [-1, 16, 32, 32]               0\n",
            "           Conv2d-12           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-13           [-1, 16, 32, 32]              32\n",
            "   BasicConvBlock-14           [-1, 16, 32, 32]               0\n",
            "           Conv2d-15           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-16           [-1, 16, 32, 32]              32\n",
            "             ReLU-17           [-1, 16, 32, 32]               0\n",
            "           Conv2d-18           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-19           [-1, 16, 32, 32]              32\n",
            "   BasicConvBlock-20           [-1, 16, 32, 32]               0\n",
            "           Conv2d-21           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-22           [-1, 16, 32, 32]              32\n",
            "             ReLU-23           [-1, 16, 32, 32]               0\n",
            "           Conv2d-24           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-25           [-1, 16, 32, 32]              32\n",
            "   BasicConvBlock-26           [-1, 16, 32, 32]               0\n",
            "           Conv2d-27           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-28           [-1, 16, 32, 32]              32\n",
            "             ReLU-29           [-1, 16, 32, 32]               0\n",
            "           Conv2d-30           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-31           [-1, 16, 32, 32]              32\n",
            "   BasicConvBlock-32           [-1, 16, 32, 32]               0\n",
            "           Conv2d-33           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-34           [-1, 16, 32, 32]              32\n",
            "             ReLU-35           [-1, 16, 32, 32]               0\n",
            "           Conv2d-36           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-37           [-1, 16, 32, 32]              32\n",
            "   BasicConvBlock-38           [-1, 16, 32, 32]               0\n",
            "           Conv2d-39           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-40           [-1, 16, 32, 32]              32\n",
            "             ReLU-41           [-1, 16, 32, 32]               0\n",
            "           Conv2d-42           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-43           [-1, 16, 32, 32]              32\n",
            "   BasicConvBlock-44           [-1, 16, 32, 32]               0\n",
            "           Conv2d-45           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-46           [-1, 16, 32, 32]              32\n",
            "             ReLU-47           [-1, 16, 32, 32]               0\n",
            "           Conv2d-48           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-49           [-1, 16, 32, 32]              32\n",
            "   BasicConvBlock-50           [-1, 16, 32, 32]               0\n",
            "           Conv2d-51           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-52           [-1, 16, 32, 32]              32\n",
            "             ReLU-53           [-1, 16, 32, 32]               0\n",
            "           Conv2d-54           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-55           [-1, 16, 32, 32]              32\n",
            "   BasicConvBlock-56           [-1, 16, 32, 32]               0\n",
            "           Conv2d-57           [-1, 32, 16, 16]           4,608\n",
            "      BatchNorm2d-58           [-1, 32, 16, 16]              64\n",
            "             ReLU-59           [-1, 32, 16, 16]               0\n",
            "           Conv2d-60           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-61           [-1, 32, 16, 16]              64\n",
            "      LambdaLayer-62           [-1, 32, 16, 16]               0\n",
            "   BasicConvBlock-63           [-1, 32, 16, 16]               0\n",
            "           Conv2d-64           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-65           [-1, 32, 16, 16]              64\n",
            "             ReLU-66           [-1, 32, 16, 16]               0\n",
            "           Conv2d-67           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-68           [-1, 32, 16, 16]              64\n",
            "   BasicConvBlock-69           [-1, 32, 16, 16]               0\n",
            "           Conv2d-70           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-71           [-1, 32, 16, 16]              64\n",
            "             ReLU-72           [-1, 32, 16, 16]               0\n",
            "           Conv2d-73           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-74           [-1, 32, 16, 16]              64\n",
            "   BasicConvBlock-75           [-1, 32, 16, 16]               0\n",
            "           Conv2d-76           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-77           [-1, 32, 16, 16]              64\n",
            "             ReLU-78           [-1, 32, 16, 16]               0\n",
            "           Conv2d-79           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-80           [-1, 32, 16, 16]              64\n",
            "   BasicConvBlock-81           [-1, 32, 16, 16]               0\n",
            "           Conv2d-82           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-83           [-1, 32, 16, 16]              64\n",
            "             ReLU-84           [-1, 32, 16, 16]               0\n",
            "           Conv2d-85           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-86           [-1, 32, 16, 16]              64\n",
            "   BasicConvBlock-87           [-1, 32, 16, 16]               0\n",
            "           Conv2d-88           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-89           [-1, 32, 16, 16]              64\n",
            "             ReLU-90           [-1, 32, 16, 16]               0\n",
            "           Conv2d-91           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-92           [-1, 32, 16, 16]              64\n",
            "   BasicConvBlock-93           [-1, 32, 16, 16]               0\n",
            "           Conv2d-94           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-95           [-1, 32, 16, 16]              64\n",
            "             ReLU-96           [-1, 32, 16, 16]               0\n",
            "           Conv2d-97           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-98           [-1, 32, 16, 16]              64\n",
            "   BasicConvBlock-99           [-1, 32, 16, 16]               0\n",
            "          Conv2d-100           [-1, 32, 16, 16]           9,216\n",
            "     BatchNorm2d-101           [-1, 32, 16, 16]              64\n",
            "            ReLU-102           [-1, 32, 16, 16]               0\n",
            "          Conv2d-103           [-1, 32, 16, 16]           9,216\n",
            "     BatchNorm2d-104           [-1, 32, 16, 16]              64\n",
            "  BasicConvBlock-105           [-1, 32, 16, 16]               0\n",
            "          Conv2d-106           [-1, 32, 16, 16]           9,216\n",
            "     BatchNorm2d-107           [-1, 32, 16, 16]              64\n",
            "            ReLU-108           [-1, 32, 16, 16]               0\n",
            "          Conv2d-109           [-1, 32, 16, 16]           9,216\n",
            "     BatchNorm2d-110           [-1, 32, 16, 16]              64\n",
            "  BasicConvBlock-111           [-1, 32, 16, 16]               0\n",
            "          Conv2d-112             [-1, 64, 8, 8]          18,432\n",
            "     BatchNorm2d-113             [-1, 64, 8, 8]             128\n",
            "            ReLU-114             [-1, 64, 8, 8]               0\n",
            "          Conv2d-115             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-116             [-1, 64, 8, 8]             128\n",
            "     LambdaLayer-117             [-1, 64, 8, 8]               0\n",
            "  BasicConvBlock-118             [-1, 64, 8, 8]               0\n",
            "          Conv2d-119             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-120             [-1, 64, 8, 8]             128\n",
            "            ReLU-121             [-1, 64, 8, 8]               0\n",
            "          Conv2d-122             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-123             [-1, 64, 8, 8]             128\n",
            "  BasicConvBlock-124             [-1, 64, 8, 8]               0\n",
            "          Conv2d-125             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-126             [-1, 64, 8, 8]             128\n",
            "            ReLU-127             [-1, 64, 8, 8]               0\n",
            "          Conv2d-128             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-129             [-1, 64, 8, 8]             128\n",
            "  BasicConvBlock-130             [-1, 64, 8, 8]               0\n",
            "          Conv2d-131             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-132             [-1, 64, 8, 8]             128\n",
            "            ReLU-133             [-1, 64, 8, 8]               0\n",
            "          Conv2d-134             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-135             [-1, 64, 8, 8]             128\n",
            "  BasicConvBlock-136             [-1, 64, 8, 8]               0\n",
            "          Conv2d-137             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-138             [-1, 64, 8, 8]             128\n",
            "            ReLU-139             [-1, 64, 8, 8]               0\n",
            "          Conv2d-140             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-141             [-1, 64, 8, 8]             128\n",
            "  BasicConvBlock-142             [-1, 64, 8, 8]               0\n",
            "          Conv2d-143             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-144             [-1, 64, 8, 8]             128\n",
            "            ReLU-145             [-1, 64, 8, 8]               0\n",
            "          Conv2d-146             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-147             [-1, 64, 8, 8]             128\n",
            "  BasicConvBlock-148             [-1, 64, 8, 8]               0\n",
            "          Conv2d-149             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-150             [-1, 64, 8, 8]             128\n",
            "            ReLU-151             [-1, 64, 8, 8]               0\n",
            "          Conv2d-152             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-153             [-1, 64, 8, 8]             128\n",
            "  BasicConvBlock-154             [-1, 64, 8, 8]               0\n",
            "          Conv2d-155             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-156             [-1, 64, 8, 8]             128\n",
            "            ReLU-157             [-1, 64, 8, 8]               0\n",
            "          Conv2d-158             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-159             [-1, 64, 8, 8]             128\n",
            "  BasicConvBlock-160             [-1, 64, 8, 8]               0\n",
            "          Conv2d-161             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-162             [-1, 64, 8, 8]             128\n",
            "            ReLU-163             [-1, 64, 8, 8]               0\n",
            "          Conv2d-164             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-165             [-1, 64, 8, 8]             128\n",
            "  BasicConvBlock-166             [-1, 64, 8, 8]               0\n",
            "AdaptiveAvgPool2d-167             [-1, 64, 1, 1]               0\n",
            "          Linear-168                    [-1, 6]             390\n",
            "================================================================\n",
            "Total params: 852,758\n",
            "Trainable params: 852,758\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 12.16\n",
            "Params size (MB): 3.25\n",
            "Estimated Total Size (MB): 15.42\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "C2r7Pc-Cbf2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "def train_model():\n",
        "    EPOCHS = 15\n",
        "    train_samples_num = 630\n",
        "    val_samples_num = 135\n",
        "    train_costs, val_costs = [], []\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        train_running_loss = 0\n",
        "        correct_train = 0\n",
        "\n",
        "        model.train().cuda()\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            prediction = model(inputs)\n",
        "            loss = criterion(prediction, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            _, predicted_outputs = torch.max(prediction.data, 1)\n",
        "            correct_train += (predicted_outputs == labels).float().sum().item()\n",
        "\n",
        "            train_running_loss += (loss.data.item() * inputs.shape[0])\n",
        "\n",
        "        train_epoch_loss = train_running_loss / train_samples_num\n",
        "        train_costs.append(train_epoch_loss)\n",
        "        train_acc = correct_train / train_samples_num\n",
        "\n",
        "        val_running_loss = 0\n",
        "        correct_val = 0\n",
        "\n",
        "        model.eval().cuda()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                prediction = model(inputs)\n",
        "                loss = criterion(prediction, labels)\n",
        "                _, predicted_outputs = torch.max(prediction.data, 1)\n",
        "                correct_val += (predicted_outputs == labels).float().sum().item()\n",
        "\n",
        "            val_running_loss += (loss.data.item() * inputs.shape[0])\n",
        "            val_epoch_loss = val_running_loss / val_samples_num\n",
        "            val_costs.append(val_epoch_loss)\n",
        "            val_acc = correct_val / val_samples_num\n",
        "\n",
        "        print(f\"[Epoch {epoch + 1}/{EPOCHS}]: train-loss = {train_epoch_loss:.6f} | train-acc = {train_acc:.3f} | val-loss = {val_epoch_loss:.6f} | val-acc = {val_acc:.3f}\")\n",
        "\n",
        "        torch.save(model.state_dict(), f'/content/checkpoint_gpu_{epoch + 1}')\n",
        "\n",
        "    torch.save(model.state_dict(), '/content/resnet-56_weights_gpu')\n",
        "\n",
        "    return train_costs, val_costs\n",
        "\n",
        "train_costs, val_costs = train_model()\n"
      ],
      "metadata": {
        "id": "pJmNM8sMapmE",
        "outputId": "6a7bf85d-1f19-4b9e-bed7-1087aac32c7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1/15]: train-loss = 1.583307 | train-acc = 0.373 | val-loss = 0.074027 | val-acc = 0.393\n",
            "[Epoch 2/15]: train-loss = 1.480552 | train-acc = 0.417 | val-loss = 0.087986 | val-acc = 0.289\n",
            "[Epoch 3/15]: train-loss = 1.402477 | train-acc = 0.422 | val-loss = 0.062517 | val-acc = 0.444\n",
            "[Epoch 4/15]: train-loss = 1.419549 | train-acc = 0.427 | val-loss = 0.054468 | val-acc = 0.407\n",
            "[Epoch 5/15]: train-loss = 1.359981 | train-acc = 0.424 | val-loss = 0.083270 | val-acc = 0.341\n",
            "[Epoch 6/15]: train-loss = 1.325599 | train-acc = 0.487 | val-loss = 0.103237 | val-acc = 0.400\n",
            "[Epoch 7/15]: train-loss = 1.338629 | train-acc = 0.483 | val-loss = 0.076869 | val-acc = 0.378\n",
            "[Epoch 8/15]: train-loss = 1.366661 | train-acc = 0.446 | val-loss = 0.080729 | val-acc = 0.422\n",
            "[Epoch 9/15]: train-loss = 1.322707 | train-acc = 0.444 | val-loss = 0.048672 | val-acc = 0.356\n",
            "[Epoch 10/15]: train-loss = 1.306893 | train-acc = 0.460 | val-loss = 0.083231 | val-acc = 0.400\n",
            "[Epoch 11/15]: train-loss = 1.282570 | train-acc = 0.473 | val-loss = 0.100168 | val-acc = 0.400\n",
            "[Epoch 12/15]: train-loss = 1.311592 | train-acc = 0.483 | val-loss = 0.073883 | val-acc = 0.341\n",
            "[Epoch 13/15]: train-loss = 1.298473 | train-acc = 0.494 | val-loss = 0.092060 | val-acc = 0.326\n",
            "[Epoch 14/15]: train-loss = 1.253217 | train-acc = 0.503 | val-loss = 0.048925 | val-acc = 0.496\n",
            "[Epoch 15/15]: train-loss = 1.227385 | train-acc = 0.522 | val-loss = 0.054444 | val-acc = 0.415\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model():\n",
        "    test_samples_num = 135\n",
        "    correct = 0\n",
        "\n",
        "    model.eval().cuda()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            prediction = model(inputs)\n",
        "            _, predicted_class = torch.max(prediction.data, 1)\n",
        "            correct += (predicted_class == labels).float().sum().item()\n",
        "\n",
        "    test_accuracy = correct / test_samples_num\n",
        "    print(f'Test accuracy: {test_accuracy:.3f}')\n",
        "\n",
        "test_model()\n"
      ],
      "metadata": {
        "id": "LnTiDU-ebh8I",
        "outputId": "acc8e65f-712a-4ec4-94d6-0c41ed37bb55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o0cPOJIAfRN4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}